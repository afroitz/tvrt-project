{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../../data/main_dataset/main_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove unused columns from dataset\n",
    "\"\"\"\n",
    "def exclude_columns(df):\n",
    "  columns_to_exclude = ['collection_time', 'sender_first_name', 'sender_last_name', 'sender_display_name', 'sender_username', 'fwd_from_user_name', 'post_author', 'is_group_elem', 'message_group_id']\n",
    "  return df.drop(columns=columns_to_exclude)\n",
    "\n",
    "\"\"\"\n",
    "Filter out messages that are replies to messages that are not in the dataset\n",
    "\"\"\"\n",
    "def filter_out_of_dataset_replies(df, df_full):\n",
    "  replies = df[df['reply_to_message_id'].notna()]\n",
    "\n",
    "  for index, row in replies.iterrows():\n",
    "    reply_to_message_id = row['reply_to_message_id']\n",
    "    chat_handle = row['chat_handle']\n",
    "    same_chat_df = df_full[df_full['chat_handle'] == chat_handle]\n",
    "\n",
    "    # if message is a reply to a message that is not in the dataset, remove it.\n",
    "    if reply_to_message_id not in same_chat_df['telegram_message_id'].values:\n",
    "      df = df.drop(index)\n",
    "\n",
    "  return df\n",
    "\n",
    "\"\"\"\n",
    "Detect languages in text\n",
    "\"\"\"\n",
    "def detect_text_langs(text):\n",
    "  if pd.isna(text):\n",
    "    return None\n",
    "  \n",
    "  text = str(text)\n",
    "\n",
    "  try:\n",
    "    return detect_langs(text)\n",
    "  except LangDetectException:\n",
    "    return None\n",
    "\n",
    "\"\"\"\n",
    "Get the combined word count of a sample\n",
    "\"\"\"\n",
    "def get_word_count(row):\n",
    "    count = 0\n",
    "    if pd.notnull(row['message_text']):\n",
    "        count += len(row['message_text'].split(\" \"))\n",
    "    if pd.notnull(row['webpage_description']):\n",
    "        count += len(row['webpage_description'].split(\" \"))\n",
    "\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "df_raw = pd.read_csv(raw_data_path, index_col=0)\n",
    "\n",
    "# preprocessing which applies to all data, so no filtering\n",
    "print(\"Preprocessing\")\n",
    "print(f\"Row count before: {len(df_raw)}\")\n",
    "\n",
    "print(\"Excluding columns...\")\n",
    "df_raw = exclude_columns(df_raw)\n",
    "\n",
    "print(\"Make messages unique by message text and webpage description...\")\n",
    "df_raw = df_raw.drop_duplicates(subset=['message_text', 'webpage_description'])\n",
    "print(f\"Row count after: {len(df_raw)}\")\n",
    "\n",
    "# add language information\n",
    "print(\"Detecting webpage description languages...\")\n",
    "df_raw['webpage_description_lang'] = df_raw['webpage_description'].apply(detect_text_langs)\n",
    "print(\"Detecting message text languages...\")\n",
    "df_raw['message_text_lang'] = df_raw['message_text'].apply(detect_text_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter detected languages for those with high confidence\n",
    "\"\"\"\n",
    "def get_confident_langs(detect_langs_result):\n",
    "  if detect_langs_result is None:\n",
    "    return []\n",
    "  return sorted([lang.lang for lang in detect_langs_result if lang.prob > 0.5])\n",
    "\n",
    "\"\"\"\n",
    "Get the unique languages of a sample\n",
    "\"\"\"\n",
    "def get_langset(row):\n",
    "  langs = set()\n",
    "  if row['webpage_description_lang']:\n",
    "    langs.update([lang.lang for lang in row['webpage_description_lang']])\n",
    "  if row['message_text_lang']:\n",
    "    langs.update([lang.lang for lang in row['message_text_lang']])\n",
    "  return list(langs)\n",
    "\n",
    "\"\"\"\n",
    "Get the unique languages of a sample with high confidence\n",
    "\"\"\"\n",
    "def get_confident_langset(row):\n",
    "  langs = set()\n",
    "  if row['webpage_description_lang']:\n",
    "    langs.update([lang.lang for lang in row['webpage_description_lang'] if lang.prob > 0.5])\n",
    "  if row['message_text_lang']:\n",
    "    langs.update([lang.lang for lang in row['message_text_lang'] if lang.prob > 0.5])\n",
    "  return list(langs)\n",
    "\n",
    "\n",
    "df_lang_filtering = df_raw.copy()\n",
    "\n",
    "df_lang_filtering[\"langset\"] = df_lang_filtering.apply(lambda row: get_langset(row), axis=1)\n",
    "df_lang_filtering[\"langset_confident\"] = df_lang_filtering.apply(lambda row: get_confident_langset(row), axis=1)\n",
    "\n",
    "# discard messages where message text is not either confidently german or german and english only\n",
    "df_lang_filtering = df_lang_filtering[df_lang_filtering.apply(lambda row: get_confident_langs(row['message_text_lang']) == ['de'] , axis=1)]\n",
    "message_text_only_german = df_lang_filtering.apply(lambda row: get_confident_langs(row['message_text_lang']) == ['de'] , axis=1)\n",
    "message_text_german_and_english = df_lang_filtering.apply(lambda row: get_confident_langs(row['message_text_lang']) == ['de', 'en'], axis=1)\n",
    "\n",
    "df_lang_filtering = df_lang_filtering[message_text_only_german | message_text_german_and_english]\n",
    "\n",
    "df_raw = df_lang_filtering.copy()\n",
    "\n",
    "print(f\"Row count after: {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating dfs for preprocessing and full dataset...\")\n",
    "df_prepro = df_raw.copy()\n",
    "df_additional = df_raw.copy()\n",
    "\n",
    "print(f\"Row count before: {len(df_prepro)}\")\n",
    "\n",
    "# remove polls\n",
    "print(\"Removing polls...\")\n",
    "df_prepro = df_prepro[df_prepro['message_media_type'] != 'MessageMediaPoll']\n",
    "print(f\"Row count after removing polls: {len(df_prepro)}\")\n",
    "\n",
    "# remove messages without text and webpage description\n",
    "print(\"Removing messages without text and webpage description...\")\n",
    "df_prepro = df_prepro[df_prepro['message_text'].notna() | df_prepro['webpage_description'].notna()]\n",
    "print(f\"Row count after removing messages without text and webpage description: {len(df_prepro)}\")\n",
    "\n",
    "# remove messages where message text consists of only a url and no webpage description is attached\n",
    "print(\"Removing messages with only a url...\")\n",
    "df_prepro = df_prepro[~(df_prepro['message_text'].str.contains('http') & ~df_prepro['message_text'].str.contains(' ') & df_prepro[\"webpage_description\"].isna())]\n",
    "print(f\"Row count after removing messages with only a url: {len(df_prepro)}\")\n",
    "\n",
    "print(\"Removing messages with only a mention...\")\n",
    "df_prepro = df_prepro[~(df_prepro['message_text'].str.contains('@') & ~df_prepro['message_text'].str.contains(' ') & df_prepro['webpage_description'].isna())]\n",
    "print(f\"Row count after removing messages with only a mention: {len(df_prepro)}\")\n",
    "\n",
    "# remove messages shorter than 15 chars which are not replies to other messages and have no webpage attached\n",
    "print(\"Removing short messages...\")\n",
    "df_prepro = df_prepro[\n",
    "  ~(\n",
    "    (df_prepro['message_text'].str.len() < 15) & \n",
    "    (df_prepro['reply_to_message_id'].isna()) & \n",
    "    (df_prepro['message_media_type'] != 'MessageMediaWebPage')\n",
    "  )\n",
    "]\n",
    "print(f\"Row count after removing short messages: {len(df_prepro)}\")\n",
    "\n",
    "print(\"Removing long messages...\")\n",
    "df_prepro['word_count'] = df_prepro.apply(get_word_count, axis=1)\n",
    "df_prepro = df_prepro[df_prepro['word_count'] < 300]\n",
    "df_prepro = df_prepro.drop(columns=['word_count'])\n",
    "print(f\"Row count after removing long messages: {len(df_prepro)}\")\n",
    "\n",
    "\n",
    "# remove messages that are replies to messages that are not in the dataset\n",
    "print(\"Removing replies to messages not in dataset...\")\n",
    "df_prepro = filter_out_of_dataset_replies(df_prepro, df_additional)\n",
    "print(f\"Row count after removing replies to messages not in dataset: {len(df_prepro)}\")\n",
    "\n",
    "# shuffle dataset\n",
    "print(\"Shuffling...\")\n",
    "df_prepro = df_prepro.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval of reply thread messages\n",
    "Retrieve messages from the unfiltered dataset to which messages in the filtered datasets are replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "From a dataframe of samples and a dataframe of additional rows, find all additional rows to which a reply is made in the samples\n",
    "\"\"\"\n",
    "def get_additional_required(samples, additional)\n",
    "  rows_to_check = pd.DataFrame(columns=samples.columns)\n",
    "  additional_required = pd.DataFrame(columns=samples.columns)\n",
    "\n",
    "  # Iterate through samples and add reply rows to rows_to_check\n",
    "  for index, row in samples.iterrows():\n",
    "      if pd.notna(row['reply_to_message_id']):\n",
    "          rows_to_check = pd.concat([rows_to_check, row.to_frame().T])\n",
    "\n",
    "  # While rows_to_check is not empty, process the rows\n",
    "  while not rows_to_check.empty:\n",
    "      row_to_check = rows_to_check.iloc[0]\n",
    "      rows_to_check = rows_to_check.iloc[1:]\n",
    "\n",
    "      # Check if row_to_check is a reply\n",
    "      if pd.notna(row_to_check['reply_to_message_id']):\n",
    "          # Look for a matching row in df_additional\n",
    "          matching_rows = additional[\n",
    "              (additional['chat_handle'] == row_to_check['chat_handle']) &\n",
    "              (additional['telegram_message_id'] == row_to_check['reply_to_message_id'])\n",
    "          ]\n",
    "\n",
    "          # If a matching row is found, add it to additional_required\n",
    "          if not matching_rows.empty:\n",
    "              additional_required = pd.concat([additional_required, matching_rows])\n",
    "\n",
    "              # Check if the matching row is a reply, add it to rows_to_check if so\n",
    "              for _, match_row in matching_rows.iterrows():\n",
    "                  if pd.notna(match_row['reply_to_message_id']):\n",
    "                      rows_to_check = pd.concat([rows_to_check, match_row.to_frame().T])\n",
    "\n",
    "  # Make additional_required unique by 'chat_handle' and 'telegram_message_id'\n",
    "  result = additional_required.drop_duplicates(subset=['chat_handle', 'telegram_message_id'])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting and exporting\n",
    "Split the filtered dataset into slices of 5000 samples each and export them and the corresponding additional messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional = get_additional_required(df_prepro, df_additional)\n",
    "\n",
    "df_prepro.to_csv('../../data/main_dataset/main_dataset-prepro.csv')\n",
    "df_additional.to_csv('../../data/main_dataset/main_dataset-additional.csv')\n",
    "\n",
    "slice_width = 5000\n",
    "curr_start = 0\n",
    "\n",
    "while curr_start < len(df_prepro):\n",
    "  print(f\"Processing slice {curr_start} - {curr_start + slice_width}...\")\n",
    "  df_prepro_slice = df_prepro[curr_start:curr_start + slice_width]\n",
    "  df_additional_slice = get_additional_required(df_prepro_slice, df_additional)\n",
    "  \n",
    "  df_prepro_slice.to_csv(f'../../data/main_dataset/main_dataset-prepro-{curr_start}-{curr_start + slice_width}.csv')\n",
    "  df_additional_slice.to_csv(f'../../data/main_dataset/main_dataset-additional-{curr_start}-{curr_start + slice_width}.csv')\n",
    "  curr_start += slice_width\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
