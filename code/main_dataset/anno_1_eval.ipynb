{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import textwrap\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from spacy.lang.de import stop_words\n",
    "import numpy as np\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "punctuations = list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read annotation data\n",
    "The code below reads all annotation files from the annotations directory and matches them to the annotated samples, creating one annotated dataframe per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_root_dir = '../../data/annotation_results/anno_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all annotations\n",
    "annotations = []\n",
    "\n",
    "# go through dirs for annotators\n",
    "for annotator_dir in os.listdir(anno_root_dir):\n",
    "    \n",
    "    # go through dataset files for each and read annotations\n",
    "    for anno_file in os.listdir(os.path.join(anno_root_dir, annotator_dir)):\n",
    "        with open(os.path.join(anno_root_dir, annotator_dir, anno_file)) as f:\n",
    "            curr_annotated_samples = json.load(f)[\"annotatedSamples\"]\n",
    "\n",
    "            for sample in curr_annotated_samples:\n",
    "                for annotation in sample[\"annotations\"]:\n",
    "                    annotations.append({\n",
    "                        \"annotator\": annotator_dir,\n",
    "                        \"dataset\": anno_file,\n",
    "                        \"sample\": sample[\"sampleIndex\"],\n",
    "                        \"rumor\": annotation[\"rumorIndex\"],\n",
    "                        \"label\": annotation[\"label\"]\n",
    "                    })\n",
    "\n",
    "anno_df = pd.DataFrame(annotations)\n",
    "\n",
    "migrant_anno_df = anno_df[anno_df[\"dataset\"] == \"tfidf_migrant.json\"]\n",
    "vacc_anno_df = anno_df[anno_df[\"dataset\"] == \"tfidf_vacc.json\"]\n",
    "brandenburg_anno_df = anno_df[anno_df[\"dataset\"] == \"tfidf_brandenburg.json\"]\n",
    "trans_anno_df = anno_df[anno_df[\"dataset\"] == \"tfidf_trans.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "migrant_df = pd.read_csv(\"../../data/main_dataset/anno_1_set-migration.csv\", index_col=0).head(150).reset_index(drop=True)\n",
    "vacc_df = pd.read_csv(\"../../data/main_dataset/anno_1_set-vaccination.csv\", index_col=0).head(150).reset_index(drop=True)\n",
    "brandenburg_df = pd.read_csv(\"../../data/main_dataset/anno_1_set-brandenburg.csv\", index_col=0).head(150).reset_index(drop=True)\n",
    "trans_df = pd.read_csv(\"../../data/main_dataset/anno_1_set-trans.csv\", index_col=0).head(150).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read task\n",
    "with open(\"../../data/tasks/task_v1.json\") as f:\n",
    "    task = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations_for_annotator(row, anno_df, annotator):\n",
    "    relevant_annotations = anno_df[(anno_df[\"sample\"] == row.name) & (anno_df[\"annotator\"] == annotator) & (anno_df[\"label\"] == \"related\")]\n",
    "    return sorted(list(set(relevant_annotations[\"rumor\"].values.tolist())))\n",
    "\n",
    "# Add annotation columns for each annotator to the datasets\n",
    "migrant_df[\"alex\"] = migrant_df.apply(lambda row: get_annotations_for_annotator(row, migrant_anno_df, \"alex\"), axis=1)\n",
    "migrant_df[\"louisa\"] = migrant_df.apply(lambda row: get_annotations_for_annotator(row, migrant_anno_df, \"louisa\"), axis=1)\n",
    "\n",
    "vacc_df[\"alex\"] = vacc_df.apply(lambda row: get_annotations_for_annotator(row, vacc_anno_df, \"alex\"), axis=1)\n",
    "vacc_df[\"louisa\"] = vacc_df.apply(lambda row: get_annotations_for_annotator(row, vacc_anno_df, \"louisa\"), axis=1)\n",
    "\n",
    "brandenburg_df[\"alex\"] = brandenburg_df.apply(lambda row: get_annotations_for_annotator(row, brandenburg_anno_df, \"alex\"), axis=1)\n",
    "brandenburg_df[\"louisa\"] = brandenburg_df.apply(lambda row: get_annotations_for_annotator(row, brandenburg_anno_df, \"louisa\"), axis=1)\n",
    "\n",
    "trans_df[\"alex\"] = trans_df.apply(lambda row: get_annotations_for_annotator(row, trans_anno_df, \"alex\"), axis=1)\n",
    "trans_df[\"louisa\"] = trans_df.apply(lambda row: get_annotations_for_annotator(row, trans_anno_df, \"louisa\"), axis=1)\n",
    "\n",
    "# Compute annotator agreements, differences and general presence of annotations\n",
    "migrant_same = migrant_df[migrant_df[\"alex\"] == migrant_df[\"louisa\"]]\n",
    "migrant_different = migrant_df[migrant_df[\"alex\"] != migrant_df[\"louisa\"]]\n",
    "migrant_any = migrant_df[migrant_df[\"alex\"].apply(lambda x: len(x) > 0) | migrant_df[\"louisa\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "vacc_same = vacc_df[vacc_df[\"alex\"] == vacc_df[\"louisa\"]]\n",
    "vacc_different = vacc_df[vacc_df[\"alex\"] != vacc_df[\"louisa\"]]\n",
    "vacc_any = vacc_df[vacc_df[\"alex\"].apply(lambda x: len(x) > 0) | vacc_df[\"louisa\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "brandenburg_same = brandenburg_df[brandenburg_df[\"alex\"] == brandenburg_df[\"louisa\"]]\n",
    "brandenburg_different = brandenburg_df[brandenburg_df[\"alex\"] != brandenburg_df[\"louisa\"]]\n",
    "brandenburg_any = brandenburg_df[brandenburg_df[\"alex\"].apply(lambda x: len(x) > 0) | brandenburg_df[\"louisa\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "trans_same = trans_df[trans_df[\"alex\"] == trans_df[\"louisa\"]]\n",
    "trans_different = trans_df[trans_df[\"alex\"] != trans_df[\"louisa\"]]\n",
    "trans_any = trans_df[trans_df[\"alex\"].apply(lambda x: len(x) > 0) | trans_df[\"louisa\"].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "total_rows_with_annotations = len(migrant_any) + len(vacc_any) + len(brandenburg_any) + len(trans_any)\n",
    "total_differences = len(migrant_different) + len(vacc_different) + len(brandenburg_different) + len(trans_different)\n",
    "\n",
    "print(f\"Total rows with any annotations: {total_rows_with_annotations}\")\n",
    "print(f\"Total differences: {total_differences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the rumor text for a given index\n",
    "\"\"\"\n",
    "def rumor_index_to_rumor(index):\n",
    "  return task[\"rumors\"][index]\n",
    "\n",
    "\"\"\"\n",
    "Print a text wrapped to fit the terminal width\n",
    "\"\"\"\n",
    "def print_wrapped_to_fit_terminal(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    width = shutil.get_terminal_size(fallback=(80, 20)).columns\n",
    "    print(textwrap.fill(text, width=width))\n",
    "\n",
    "\"\"\"\n",
    "Show differences between two annotators.\n",
    "Example: show_differences(migrant_df, \"alex\", \"louisa\")\n",
    "\"\"\"\n",
    "def show_differences(df, annotator_1, annotator_2):\n",
    "  for i, row in df.iterrows():\n",
    "    if row[annotator_1] != row[annotator_2]:\n",
    "      print(\"---\")\n",
    "      print(i)\n",
    "      print(row[\"message_date\"])\n",
    "      print_wrapped_to_fit_terminal(row[\"text\"])\n",
    "      print(\"Alex: \", [rumor_index_to_rumor(rumor) for rumor in row[\"alex\"]])\n",
    "      print(\"Louisa: \", [rumor_index_to_rumor(rumor) for rumor in row[\"louisa\"]])\n",
    "      print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference correction\n",
    "The following code adds the annotation changes agreed on during the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migrant_df_after_discussion = migrant_df.copy()\n",
    "migrant_df_after_discussion.at[1, \"louisa\"] = migrant_df_after_discussion.at[1, \"alex\"]\n",
    "migrant_df_after_discussion.at[19, \"louisa\"] = migrant_df_after_discussion.at[19, \"alex\"]\n",
    "migrant_df_after_discussion.at[31, \"alex\"] = migrant_df_after_discussion.at[31, \"louisa\"]\n",
    "migrant_df_after_discussion.at[32, \"alex\"] = migrant_df_after_discussion.at[32, \"louisa\"]\n",
    "migrant_df_after_discussion.at[55, \"louisa\"] = migrant_df_after_discussion.at[55, \"alex\"]\n",
    "migrant_df_after_discussion.at[63, \"alex\"] = migrant_df_after_discussion.at[63, \"louisa\"]\n",
    "migrant_df_after_discussion.at[64, \"louisa\"] = migrant_df_after_discussion.at[64, \"alex\"]\n",
    "migrant_df_after_discussion.at[69, \"alex\"] = migrant_df_after_discussion.at[69, \"louisa\"]\n",
    "migrant_df_after_discussion.at[124, \"louisa\"] = migrant_df_after_discussion.at[124, \"alex\"]\n",
    "migrant_df_after_discussion.at[140, \"alex\"] = migrant_df_after_discussion.at[140, \"louisa\"]\n",
    "\n",
    "trans_df_after_discussion = trans_df.copy()\n",
    "trans_df_after_discussion.at[4, \"louisa\"] = trans_df_after_discussion.at[4, \"alex\"]\n",
    "trans_df_after_discussion.at[14, \"alex\"] = trans_df_after_discussion.at[14, \"louisa\"]\n",
    "trans_df_after_discussion.at[65, \"louisa\"] = trans_df_after_discussion.at[65, \"alex\"]\n",
    "trans_df_after_discussion.at[99, \"louisa\"] = trans_df_after_discussion.at[99, \"alex\"]\n",
    "trans_df_after_discussion.at[123, \"louisa\"] = trans_df_after_discussion.at[123, \"alex\"]\n",
    "trans_df_after_discussion.at[135, \"louisa\"] = trans_df_after_discussion.at[135, \"alex\"]\n",
    "trans_df_after_discussion.at[138, \"louisa\"] = trans_df_after_discussion.at[138, \"alex\"]\n",
    "\n",
    "brandenburg_df_after_discussion = brandenburg_df.copy()\n",
    "brandenburg_df_after_discussion.at[14, \"louisa\"] = brandenburg_df_after_discussion.at[14, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[19, \"louisa\"] = brandenburg_df_after_discussion.at[19, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[28, \"alex\"] = [0]\n",
    "brandenburg_df_after_discussion.at[28, \"louisa\"] = [0]\n",
    "brandenburg_df_after_discussion.at[33, \"louisa\"] = brandenburg_df_after_discussion.at[33, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[46, \"louisa\"] = brandenburg_df_after_discussion.at[46, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[47, \"louisa\"] = brandenburg_df_after_discussion.at[47, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[55, \"louisa\"] = brandenburg_df_after_discussion.at[55, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[58, \"alex\"] = brandenburg_df_after_discussion.at[58, \"alex\"] + brandenburg_df_after_discussion.at[58, \"louisa\"]\n",
    "brandenburg_df_after_discussion.at[59, \"alex\"] = brandenburg_df_after_discussion.at[59, \"alex\"] + brandenburg_df_after_discussion.at[59, \"louisa\"]\n",
    "brandenburg_df_after_discussion.at[89, \"louisa\"] = brandenburg_df_after_discussion.at[89, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[105, \"alex\"] = brandenburg_df_after_discussion.at[105, \"louisa\"]\n",
    "brandenburg_df_after_discussion.at[111, \"louisa\"] = [1]\n",
    "brandenburg_df_after_discussion.at[112, \"alex\"] = [0]\n",
    "brandenburg_df_after_discussion.at[112, \"louisa\"] = [0]\n",
    "brandenburg_df_after_discussion.at[117, \"louisa\"] = brandenburg_df_after_discussion.at[117, \"alex\"]\n",
    "brandenburg_df_after_discussion.at[124, \"alex\"] = brandenburg_df_after_discussion.at[124, \"louisa\"]\n",
    "brandenburg_df_after_discussion.at[133, \"alex\"] = brandenburg_df_after_discussion.at[133, \"louisa\"]\n",
    "brandenburg_df_after_discussion.at[136, \"louisa\"] = brandenburg_df_after_discussion.at[136, \"louisa\"] + [1]\n",
    "\n",
    "vacc_df_after_discussion = vacc_df.copy()\n",
    "vacc_df_after_discussion.at[5, \"alex\"] = vacc_df_after_discussion.at[5, \"louisa\"]\n",
    "vacc_df_after_discussion.at[23, \"louisa\"] = vacc_df_after_discussion.at[23, \"alex\"]\n",
    "vacc_df_after_discussion.at[25, \"louisa\"] = vacc_df_after_discussion.at[25, \"alex\"]\n",
    "vacc_df_after_discussion.at[31, \"alex\"] = vacc_df_after_discussion.at[31, \"louisa\"]\n",
    "vacc_df_after_discussion.at[68, \"louisa\"] = vacc_df_after_discussion.at[68, \"alex\"]\n",
    "vacc_df_after_discussion.at[129, \"louisa\"] = vacc_df_after_discussion.at[129, \"alex\"]\n",
    "vacc_df_after_discussion.at[135, \"louisa\"] = vacc_df_after_discussion.at[135, \"alex\"]\n",
    "vacc_df_after_discussion.at[148, \"alex\"] = vacc_df_after_discussion.at[148, \"louisa\"]\n",
    "\n",
    "\n",
    "all_after_discussion = pd.concat([migrant_df_after_discussion, vacc_df_after_discussion, brandenburg_df_after_discussion, trans_df_after_discussion])\n",
    "all_differences = len(all_after_discussion[all_after_discussion[\"alex\"] != all_after_discussion[\"louisa\"]])\n",
    "\n",
    "equal_vals_after_discussion = all_after_discussion[all_after_discussion[\"alex\"] == all_after_discussion[\"louisa\"]][\"alex\"].values.tolist()\n",
    "equal_vals_after_discussion = [item for sublist in equal_vals_after_discussion for item in sublist]\n",
    "rumor_counts_after_discussion = pd.Series(equal_vals_after_discussion).value_counts()\n",
    "\n",
    "# Print info about the distribution of rumors after discussion\n",
    "print(\"RUMOR DISTRIBUTION AFTER DISCUSSION:\")\n",
    "for rumor_index, count in rumor_counts_after_discussion.items():\n",
    "    print(f\"{rumor_index_to_rumor(rumor_index)}: {count}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Print info about the distribution of rumors before discussion\n",
    "differences_after_discussion = all_after_discussion[all_after_discussion[\"alex\"] != all_after_discussion[\"louisa\"]]\n",
    "print(f\"Differences after discussion: {len(differences_after_discussion)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Print info about the distribution of rumors involved in unresolved differences\n",
    "different_vals_after_discussion = differences_after_discussion[\"alex\"].values.tolist() + differences_after_discussion[\"louisa\"].values.tolist()\n",
    "different_vals_after_discussion = [item for sublist in different_vals_after_discussion for item in sublist]\n",
    "diff_rumor_counts_after_discussion = pd.Series(different_vals_after_discussion).value_counts()\n",
    "\n",
    "print(\"RUMORS INVOLVED IN UNRESOLVED DIFFERENCES:\")\n",
    "for rumor_index, count in diff_rumor_counts_after_discussion.items():\n",
    "    print(f\"{rumor_index_to_rumor(rumor_index)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and exporting the final annotated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df[\"alex_after\"] = trans_df_after_discussion[\"alex\"]\n",
    "trans_df[\"louisa_after\"] = trans_df_after_discussion[\"louisa\"]\n",
    "vacc_df[\"alex_after\"] = vacc_df_after_discussion[\"alex\"]\n",
    "vacc_df[\"louisa_after\"] = vacc_df_after_discussion[\"louisa\"]\n",
    "brandenburg_df[\"alex_after\"] = brandenburg_df_after_discussion[\"alex\"]\n",
    "brandenburg_df[\"louisa_after\"] = brandenburg_df_after_discussion[\"louisa\"]\n",
    "migrant_df[\"alex_after\"] = migrant_df_after_discussion[\"alex\"]\n",
    "migrant_df[\"louisa_after\"] = migrant_df_after_discussion[\"louisa\"]\n",
    "all_df = pd.concat([migrant_df, vacc_df, brandenburg_df, trans_df])\n",
    "\n",
    "all_df[\"agree_after\"] = all_df[\"alex_after\"] == all_df[\"louisa_after\"]\n",
    "\n",
    "# export full annotated dataset\n",
    "all_df.to_csv(\"../../data/annotation_results/anno_01_annotated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental negative lemma research\n",
    "The code below is an experiment to find lemmas that are negatively correlated with positive samples, i.e. samples annotated as containing one of the rumors. This was later used to filter out off-topic posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the text\n",
    "punctuations += [\"\\u2796\", \"--\", \"ⓜ\", \"⚡\", \"nan\", \"\\uFEFF\"]\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = nlp(text)\n",
    "    # lemmatizing\n",
    "    sentence = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text ]\n",
    "    # removing stop words\n",
    "    sentence = [ word for word in sentence if word not in stop_words and word not in punctuations ]        \n",
    "    return sentence\n",
    "\n",
    "\n",
    "all_df[\"lemma\"] = all_df[\"text\"].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of all lemmas from agreed and from positive samples as well as frequency counts\n",
    "valid = all_df[all_df[\"agree_after\"] == True]\n",
    "positive = valid[valid[\"alex_after\"].apply(lambda x: len(x) > 0)]\n",
    "positive_lemma = [lemma for text in positive[\"lemma\"].values.tolist() for lemma in list(set(text))]\n",
    "all_lemma = [lemma for text in valid[\"lemma\"].values.tolist() for lemma in list(set(text))]\n",
    "\n",
    "positive_lemma_counts = pd.Series(positive_lemma).value_counts()\n",
    "all_lemma_counts = pd.Series(all_lemma).value_counts()\n",
    "\n",
    "# Get total number of lemmas in positive and valid sets and vocabulary size\n",
    "total_pos = sum(positive_lemma_counts.values)\n",
    "total_valid = sum(all_lemma_counts.values)\n",
    "unique_lemma = len(all_lemma_counts)\n",
    "\n",
    "alpha = 0.001 # smoothing parameter\n",
    "\n",
    "\n",
    "log_odds_ratios = {}\n",
    "\n",
    "# Iterate over vocabulary\n",
    "for token in all_lemma_counts.index:\n",
    "    # Get counts for the token in positive and valid sets\n",
    "    pos_count = positive_lemma_counts.get(token, 0)\n",
    "    valid_count = all_lemma_counts.get(token, 0)\n",
    "\n",
    "    # Compute token probabilities\n",
    "    p_token_pos = (pos_count + alpha) / (total_pos + alpha * unique_lemma)\n",
    "    p_token_valid = (valid_count + alpha) / (total_valid + alpha * unique_lemma)\n",
    "\n",
    "    # Compute log-odds ratio\n",
    "    log_odds_ratio = np.log(p_token_pos / p_token_valid)\n",
    "    log_odds_ratios[token] = log_odds_ratio\n",
    "\n",
    "# Convert log-odds to dataframe\n",
    "log_odds_df = pd.DataFrame({\n",
    "    'token': list(log_odds_ratios.keys()),\n",
    "    'log_odds': list(log_odds_ratios.values()),\n",
    "}).sort_values(by='log_odds', ascending=True)\n",
    "\n",
    "# Add count: How often does lemma appear in positive and valid sets?\n",
    "log_odds_df[\"positive_count\"] = log_odds_df[\"token\"].apply(lambda x: positive_lemma_counts.get(x, 0))\n",
    "log_odds_df[\"valid_count\"] = log_odds_df[\"token\"].apply(lambda x: all_lemma_counts.get(x, 0))\n",
    "\n",
    "# Inspect tokens with most negative log-odds ratios\n",
    "print(\"Most negatively correlated tokens:\")\n",
    "for i, row in log_odds_df.head(200).iterrows():\n",
    "    print(f\"{row['token']}: {row['positive_count']} / {row['valid_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
