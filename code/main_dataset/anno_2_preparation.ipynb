{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import spacy\n",
    "from string import punctuation\n",
    "from spacy.lang.de import stop_words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "punctuations = list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv('../../data/main_dataset/main_dataset-prepro-5000-10000.csv')\n",
    "\n",
    "# read the annotation results from iteration one and create a dataframe of only positive examples\n",
    "anno_1_results = pd.read_csv('../../data/annotation_results/anno_01_annotated.csv', index_col=0)\n",
    "anno_1_positive = anno_1_results[(anno_1_results[\"alex_after\"] == anno_1_results[\"louisa_after\"]) & (anno_1_results[\"alex_after\"].apply(ast.literal_eval).apply(len) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the text of a sample\n",
    "\"\"\"\n",
    "def get_row_text(row):\n",
    "  text = ''\n",
    "\n",
    "  if type(row['message_text']) == str:\n",
    "    text += row['message_text']\n",
    "  if type(row['webpage_description']) == str:\n",
    "    text += row['webpage_description']\n",
    "\n",
    "  return text\n",
    "\n",
    "anno_1_positive[\"text\"] = anno_1_positive.apply(get_row_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_lemmas = [\"\\u2796\", \"--\", \"ⓜ\", \"⚡\", \"nan\", \"\\uFEFF\"]\n",
    "\n",
    "def lemmatize(row):\n",
    "  text = get_row_text(row)\n",
    "\n",
    "  text = nlp(text)\n",
    "  # lemmatizing\n",
    "  sentence = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text ]\n",
    "  # removing stop words\n",
    "  sentence = [ word for word in sentence if word not in stop_words and word not in punctuations and word not in stop_lemmas ]        \n",
    "  return sentence\n",
    "\n",
    "df[\"lemma\"] = df.apply(lemmatize, axis=1)\n",
    "anno_1_positive[\"lemma\"] = anno_1_positive.apply(lemmatize, axis=1)\n",
    "df[\"lemma_joined\"] = df[\"lemma\"].apply(lambda x: \" \".join(x))\n",
    "anno_1_positive[\"lemma_joined\"] = anno_1_positive[\"lemma\"].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove negatively associated lemmas\n",
    "The following code removes samples containing lemmas which were found to be negatively correlated with the rumors (see anno_1_eval.ipynb). This is meant to remove off-topic samples, mostly related to geopolitics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"russland\", \"usa\", \"israel\", \"trump\", \"biden\", \"harris\", \"hisbollah\", \"libanon\", \"amerikanisch\", \"amerika\", \"militärisch\", \"militär\", \"ukraine\", \"ukrainisch\", \"russisch\", \"inflation\", \"klimaschutz\", \"euro\", \"diddy\", \"putin\", \"krieg\", \"soldat\", \"nasrallah\", \"illuminat\"]\n",
    "\n",
    "df_filtered = df.copy()\n",
    "\n",
    "for keyword in keywords:\n",
    "  len_before = len(df_filtered)\n",
    "  df_filtered = df_filtered[df_filtered[\"lemma\"].apply(lambda x: keyword not in x)]\n",
    "  print(f\"'{keyword}': {len_before - len(df_filtered)}\")\n",
    "\n",
    "print(\"Remaining:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create annotation set\n",
    "The below code computes the cosine similarity between the tf-idf vectorized positive examples from the previous iteration and the tf-idf vectorized lemmatized dataset. For each sample, the highest similarity to any positive example is taken, ranking the dataset by this similarity. An annotation dataset consisting of the top 400 most similar as well as 200 random samples is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 600\n",
    "tfidf_ratio = 0.75\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(pd.concat([df_filtered[\"lemma_joined\"], anno_1_positive[\"lemma_joined\"]]))\n",
    "\n",
    "samples_tfidf = vectorizer.transform(df_filtered[\"lemma_joined\"])\n",
    "positive_tfidf = vectorizer.transform(anno_1_positive[\"lemma_joined\"])\n",
    "\n",
    "cosine_similarities = cosine_similarity(samples_tfidf, positive_tfidf)\n",
    "\n",
    "av_similarities = np.mean(cosine_similarities, axis=1)\n",
    "max_similarities = np.max(cosine_similarities, axis=1)\n",
    "\n",
    "sorted_indices = np.argsort(-max_similarities)\n",
    "sorted_df = df_filtered.iloc[sorted_indices]\n",
    "\n",
    "tf_idf_samples = sorted_df.head(int(total_samples * tfidf_ratio))\n",
    "remaining_samples = sorted_df.tail(int(total_samples * (1 - tfidf_ratio))).sample(total_samples - int(total_samples * tfidf_ratio))\n",
    "\n",
    "final_df = pd.concat([tf_idf_samples, remaining_samples]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/main_dataset/anno_2_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create additional rumors annotation set\n",
    "In the below code, possible examples of two new rumors introduced in the second annotation round are searched, creating an additional annotation set specifically for these rumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_covid_keywords = [\"long\", \"long-covid\", \"lauterbach\", \"gesundheitsminister\"]\n",
    "\n",
    "vacc_danger_keywords = [\n",
    "    \"schweigekartell\", \"impfschaede\", \"impfnebenwirkung\", \"impfgeschädigte\",\n",
    "    \"impfschade\", \"impfopfer\", \"impfstaat\", \"impfschäde\", \"impfzwang\",\n",
    "    \"impfschaed\", \"impfschäden\", \"impfschaden\", \"dunkelziffer\", \"berninger\",\n",
    "    \"mut-ärztin\", \"mut-arzt\", \"mut-psychologin\", \"mut-ärzt\", \"mut-politiker\",\n",
    "    \"mut\", \"nebenwirkung\", \"nebenwirkungsfrei\"\n",
    "]\n",
    "\n",
    "# Filter out the samples that contain the keywords from both sets\n",
    "long_covid_df = df_filtered[df_filtered[\"lemma\"].apply(lambda x: any([keyword in x for keyword in long_covid_keywords]))]\n",
    "vacc_danger_df = df_filtered[df_filtered[\"lemma\"].apply(lambda x: any([keyword in x for keyword in vacc_danger_keywords]))]\n",
    "anno_2_keywords_df = pd.concat([long_covid_df, vacc_danger_df]).drop_duplicates(subset=[\"chat_handle\", \"telegram_message_id\"])\n",
    "\n",
    "# Drop duplicates\n",
    "merged_df = anno_2_keywords_df.merge(final_df[['chat_handle', 'telegram_message_id']], on=['chat_handle', 'telegram_message_id'], how='left', indicator=True)\n",
    "result_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Export data\n",
    "result_df.to_csv('../../data/main_dataset/anno_2_set-additional.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
