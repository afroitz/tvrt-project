{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from string import punctuation\n",
    "from spacy.lang.de import stop_words\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "stop_words = stop_words.STOP_WORDS\n",
    "punctuations = list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "- Samples for third annotation iteration\n",
    "- Positive samples from second iteration, cleaned, meaning message footers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/main_dataset/main_dataset-prepro-10000-15000.csv')\n",
    "positive_cleaned = pd.read_csv('../../data/annotation_results/anno_02_positive-cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the text of a sample\n",
    "\"\"\"\n",
    "def get_row_text(row):\n",
    "    text = \"\"\n",
    "    if type(row[\"message_text\"]) == str:\n",
    "        text += row[\"message_text\"]\n",
    "\n",
    "    if type(row[\"webpage_description\"]) == str:\n",
    "        text += row[\"webpage_description\"]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_lemmas = [\"\\u2796\", \"--\", \"ⓜ\", \"⚡\", \"nan\", \"\\uFEFF\"]\n",
    "\n",
    "def lemmatize(row):\n",
    "  text = get_row_text(row)\n",
    "\n",
    "  text = nlp(text)\n",
    "  # lemmatizing\n",
    "  sentence = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text ]\n",
    "  # removing stop words\n",
    "  sentence = [ word for word in sentence if not word in stop_words ] \n",
    "  sentence = [ word for word in sentence if not word in punctuations ]\n",
    "  sentence = [word for word in sentence if not word in stop_lemmas]\n",
    "  sentence = [word for word in sentence if not \"t.me\" in word]\n",
    "  sentence = [word for word in sentence if not \"http\" in word]\n",
    "  sentence = [word for word in sentence if not \"www\" in word]\n",
    "  sentence = [word for word in sentence if not \"@\" in word]\n",
    "  sentence = [word for word in sentence if not \".html\" in word]\n",
    "  sentence = [word for word in sentence if not \"utm_source\" in word]\n",
    "  sentence = [word for word in sentence if re.search(\"[A-Za-z0-9]\", word)]\n",
    "  sentence = [word for word in sentence if len(word) > 1]\n",
    "\n",
    "  return sentence\n",
    "\n",
    "df[\"lemma\"] = df.apply(lemmatize, axis=1)\n",
    "positive_cleaned[\"lemma\"] = positive_cleaned.apply(lemmatize, axis=1)\n",
    "df[\"lemma_joined\"] = df[\"lemma\"].apply(lambda x: \" \".join(x))\n",
    "positive_cleaned[\"lemma_joined\"] = positive_cleaned[\"lemma\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove negatively associated lemmas\n",
    "The following code removes samples containing lemmas which were found to be negatively correlated with the rumors (see anno_1_eval.ipynb). This is meant to remove off-topic samples, mostly related to geopolitics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"russland\", \"usa\", \"israel\", \"trump\", \"biden\", \"harris\", \"hisbollah\", \"libanon\", \"amerikanisch\", \"amerika\", \"militärisch\", \"militär\", \"ukraine\", \"ukrainisch\", \"russisch\", \"inflation\", \"klimaschutz\", \"euro\", \"diddy\", \"putin\", \"krieg\", \"soldat\", \"nasrallah\", \"illuminat\"]\n",
    "\n",
    "df_filtered = df.copy()\n",
    "\n",
    "for keyword in keywords:\n",
    "  len_before = len(df_filtered)\n",
    "  df_filtered = df_filtered[df_filtered[\"lemma\"].apply(lambda x: keyword not in x)]\n",
    "\n",
    "  print(f\"'{keyword}': {len_before - len(df_filtered)}\")\n",
    "\n",
    "print(\"Remaining:\", len(df_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create annotation set\n",
    "The below code computes the cosine similarity between the tf-idf vectorized positive examples from the previous iteration and the tf-idf vectorized lemmatized dataset. For each sample, the average of the three highest similarities to positive samples is taken, ranking the dataset by this similarity. An annotation dataset consisting of the top 400 most similar as well as 200 random samples is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 600\n",
    "tfidf_ratio = 0.75\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(pd.concat([df_filtered[\"lemma_joined\"], positive_cleaned[\"lemma_joined\"]]))\n",
    "\n",
    "samples_tfidf = vectorizer.transform(df_filtered[\"lemma_joined\"])\n",
    "positive_cleaned_tfidf = vectorizer.transform(positive_cleaned[\"lemma_joined\"])\n",
    "\n",
    "cosine_similarities = cosine_similarity(samples_tfidf, positive_cleaned_tfidf)\n",
    "sorted_similarities = np.sort(cosine_similarities, axis=1)[:, ::-1]\n",
    "top3_av_similarities = np.mean(sorted_similarities[:, :3], axis=1)\n",
    "\n",
    "\n",
    "sorted_indices = np.argsort(-top3_av_similarities)\n",
    "sorted_df = df_filtered.iloc[sorted_indices]\n",
    "\n",
    "tf_idf_samples = sorted_df.head(int(total_samples * tfidf_ratio))\n",
    "remaining_samples = sorted_df.tail(int(total_samples * (1 - tfidf_ratio))).sample(total_samples - int(total_samples * tfidf_ratio))\n",
    "\n",
    "final_df = pd.concat([tf_idf_samples, remaining_samples]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../../data/main_dataset/anno_3_set.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
