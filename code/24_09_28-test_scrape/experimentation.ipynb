{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "import shutil\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/24_09_28-test_scrape/24_09_28-test_scrape-prepro.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped_to_fit_terminal(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    width = shutil.get_terminal_size(fallback=(80, 20)).columns\n",
    "    print(textwrap.fill(text, width=width))\n",
    "\n",
    "\n",
    "def filter_by_keywords(df, column, keywords={}, ignore_case=True):\n",
    "    query = pd.Series([True] * len(df))  # Start with all True\n",
    "\n",
    "    if \"all\" in keywords:\n",
    "        for keyword in keywords[\"all\"]:\n",
    "            query &= df[column].str.contains(keyword, na=False, case=ignore_case)\n",
    "\n",
    "    if \"any\" in keywords:\n",
    "        some_query = pd.Series([False] * len(df))\n",
    "        for keyword in keywords[\"any\"]:\n",
    "            some_query |= df[column].str.contains(keyword, na=False, case=ignore_case)\n",
    "        query &= some_query\n",
    "\n",
    "    if \"none\" in keywords:\n",
    "        for keyword in keywords[\"none\"]:\n",
    "            query &= ~df[column].str.contains(keyword, na=False, case=ignore_case)\n",
    "\n",
    "    return df[query]\n",
    "\n",
    "\n",
    "# Filter a dataframe of messages by keywords in the message text, webpage title, and webpage description\n",
    "def filter_message_data_by_keywords(df, keywords={}, ignore_case=True):\n",
    "    column_filtered = [\n",
    "        filter_by_keywords(df, \"message_text\", keywords, ignore_case),\n",
    "        filter_by_keywords(df, \"webpage_title\", keywords, ignore_case),\n",
    "        filter_by_keywords(df, \"webpage_description\", keywords, ignore_case),\n",
    "    ]\n",
    "\n",
    "    return pd.concat(column_filtered).drop_duplicates()\n",
    "\n",
    "\n",
    "# Returns a list where each element holds matches for one row of a dataframe for a regex keyword in the columns message_text, webpage_title, and webpage_description of a dataframe\n",
    "def get_matches_for_keyword_in_message_data(df, keyword, ignore_case=True):\n",
    "    def find_matches(row):\n",
    "        message_text = row.get(\"message_text\", \"\")\n",
    "        webpage_description = row.get(\"webpage_description\", \"\")\n",
    "        webpage_title = row.get(\"webpage_title\", \"\")\n",
    "\n",
    "        all_matches = []\n",
    "        \n",
    "        # Search for matches in message_text\n",
    "        message_matches = list(re.finditer(keyword, message_text, re.IGNORECASE)) if ignore_case else list(re.finditer(keyword, message_text))\n",
    "        for match in message_matches:\n",
    "            start = max(0, match.start() - 20)\n",
    "            end = min(len(message_text), match.end() + 20)\n",
    "            all_matches.append(message_text[start:end])\n",
    "\n",
    "        # Search for matches in webpage_description\n",
    "        webp_matches = list(re.finditer(keyword, webpage_description, re.IGNORECASE)) if ignore_case else list(re.finditer(keyword, webpage_description))\n",
    "        for match in webp_matches:\n",
    "            start = max(0, match.start() - 20)\n",
    "            end = min(len(webpage_description), match.end() + 20)\n",
    "            all_matches.append(webpage_description[start:end])\n",
    "\n",
    "        # Search for matches in webpage_title\n",
    "        webp_title_matches = list(re.finditer(keyword, webpage_title, re.IGNORECASE)) if ignore_case else list(re.finditer(keyword, webpage_title))\n",
    "        for match in webp_title_matches:\n",
    "            start = max(0, match.start() - 20)\n",
    "            end = min(len(webpage_title), match.end() + 20)\n",
    "            all_matches.append(webpage_title[start:end])\n",
    "\n",
    "        return all_matches\n",
    "\n",
    "    # Apply the find_matches function to each row\n",
    "    df[\"all_matches\"] = df.apply(find_matches, axis=1)\n",
    "    \n",
    "    return df[\"all_matches\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greta_keywords = {\n",
    "  \"any\": ['[Tt]hunberg', '[Gg]reta']\n",
    "}\n",
    "df_greta = filter_message_data_by_keywords(df, keywords=greta_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_keywords = {\n",
    "  \"any\": [\n",
    "      'trans(?!(port|form|paren|pir|action|it|kript|atlanti|fer|fusion|aktion|human|cript|lat|nation|nistri))',\n",
    "      '(?<!(undle|vorra|leidi))(?<!(fol|rre|dri|ewe|sor|tra|tei))gender',\n",
    "      'binär',\n",
    "      'binary',\n",
    "      'geschlecht',\n",
    "      'queer',\n",
    "      'lgbt'\n",
    "      ]\n",
    "}\n",
    "df_trans = filter_message_data_by_keywords(df, keywords=trans_keywords)\n",
    "\n",
    "\n",
    "# '(trans(?!(port|form|paren|pir|action|it|kript|atlanti|fer|fusion|aktion|human|cript|lat|nation|nistri))|(?<!(undle|vorra|leidi))(?<!(fol|rre|dri|ewe|sor|tra|tei))gender|binär|binary|geschlecht|queer|lgbt)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migrant_keywords = {\n",
    "  \"all\": [\n",
    "     '(migrant|asyl|flücht|refug|ausländ|türk|arab|kanak|muslim|islam|terror)'\n",
    "     #'messer'\n",
    "  ]\n",
    "}\n",
    "df_migrant = filter_message_data_by_keywords(df, keywords=migrant_keywords)\n",
    "\n",
    "# '(migrant|asyl|flücht|refug|ausländ|türk|arab|kanak|muslim|islam|terror)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brandenburg_election_keywords = {\n",
    "   \"any\": ['brandenburg']\n",
    "}\n",
    "\n",
    "df_brandenburg_election = filter_message_data_by_keywords(df, keywords=brandenburg_election_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weidel_keywords = {\n",
    "  \"any\": ['weidel']\n",
    "}\n",
    "\n",
    "weidel_df = filter_message_data_by_keywords(df, keywords=weidel_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_keywords = {\n",
    "  \"any\": [\"corona\", \"covid\",\"(?<!(sch|ngl))impf\",\"vacc\",\"pandem\", \"mrna\", \"spritze\"]\n",
    "}\n",
    "\n",
    "vaccine_df = filter_message_data_by_keywords(df, keywords=vaccine_keywords)\n",
    "\n",
    "# '(corona|covid|(?<!(sch|ngl))impf|vacc|pandem|mrna|spritze)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(vaccine_df)\n",
    "\n",
    "for _, row in vaccine_df.iterrows():\n",
    "    print_wrapped_to_fit_terminal(row['message_text'])\n",
    "    print_wrapped_to_fit_terminal(row['webpage_description'])\n",
    "    print(row['message_date'])\n",
    "\n",
    "    message_text = row['message_text'] if isinstance(row['message_text'], str) else str(row['message_text'])\n",
    "    webpage_description = row['webpage_description'] if isinstance(row['webpage_description'], str) else str(row['webpage_description'])\n",
    "\n",
    "    all_matches = []\n",
    "\n",
    "    for keyword in vaccine_keywords['any']:\n",
    "        message_matches = list(re.finditer(keyword, message_text))\n",
    "        webp_matches = list(re.finditer(keyword, webpage_description))\n",
    "\n",
    "        for match in message_matches:\n",
    "          start = max(0, match.start() - 20)\n",
    "          end = min(len(row['message_text']), match.end() + 20)\n",
    "          all_matches.append(row['message_text'][start:end])\n",
    "\n",
    "        for match in webp_matches:\n",
    "          start = max(0, match.start() - 20)\n",
    "          end = min(len(row['webpage_description']), match.end() + 20)\n",
    "          all_matches.append(row['webpage_description'][start:end])\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"MATCHES:\")\n",
    "    for match in all_matches:\n",
    "      print(match)\n",
    "                                     \n",
    "    print('---')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
